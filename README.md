# üöÄ Proyecto Final SIS313: Sistema distribuido de respaldos autom√°ticos

> **Asignatura:** SIS313: Infraestructura, Plataformas Tecnol√≥gicas y Redes  
> **Semestre:** 2/2025  
> **Docente:** Ing. Marcelo Quispe Ortega  

---

## üë• Miembros del Equipo (Grupo)

| Nombre Completo | Rol en el Proyecto | Contacto (GitHub / Email) |
|-----------------|--------------------|----------------------------|
| **David Flores Padilla** | Base de Datos Maestro y Esclavo | [DavidFloresPadilla](https://github.com/DavidFloresPadilla) ¬∑ elflores43@gmail.com |
| **Hamilton Randall Almanza Castro** | Servidor de Backups | [HamiltonAlmanza](https://github.com/HamiltonAlmanza) ¬∑ hac.0000009@gmail.com |
| **Cristhian Daniel Mamani Morales** | Servidor Web 1 y Servidor Web 2 | [danielecrac736](https://github.com/danielecrac736) ¬∑ danielelcrac736@gmail.com |
| **Mateo Gabriel Villegas Arancibia** | Balanceador  | [MatVan23](https://github.com/MatVan23) ¬∑ mateorito1@gmail.com |

## I. Objetivo del Proyecto
El objetivo principal de este proyecto es crear un sistema que realice respaldos de manera autom√°tica, evitando que la informaci√≥n se pierda si ocurre alg√∫n problema en el servidor.  
La idea es que el sistema trabaje solo, sin depender de una persona para hacer copias manuales, y que la informaci√≥n est√© siempre disponible cuando se necesite.  
En pocas palabras, lo que buscamos es tener un sistema m√°s seguro, m√°s confiable y capaz de recuperarse r√°pidamente ante una falla.

---

## II. Justificaci√≥n e Importancia
Este proyecto es importante porque ayuda a que la informaci√≥n y los servicios no se pierdan ni se detengan ante una falla, un apag√≥n o un error del sistema.  
En una infraestructura universitaria o empresarial, la p√©rdida de datos puede afectar clases, tr√°mites, investigaciones o incluso procesos administrativos completos, por lo que contar con respaldos autom√°ticos y monitoreo constante se vuelve indispensable.  
La implementaci√≥n de este sistema reduce el riesgo de que todo dependa de un solo servidor (el famoso Single Point of Failure) y mejora la continuidad operacional, ya que aunque uno falle, la informaci√≥n sigue estando disponible gracias a los respaldos y al monitoreo.  

---

## III. Tecnolog√≠as y Conceptos Implementados

### 3.1 Tecnolog√≠as Clave Utilizadas

| Tecnolog√≠a | Rol dentro del proyecto |
|---|---|
| **Nginx (Tecnolog√≠a 1)** | Se us√≥ como punto central para recibir solicitudes, servir como proxy y mejorar la forma en que se entregan los servicios. |
| **PostgreSQL (Tecnolog√≠a 2)** | Base de datos principal con estructura Maestro‚ÄìEsclavo para mantener una copia id√©ntica de la informaci√≥n y protegerla contra fallos. |
| **Bash/Scripting autom√°tico (Tecnolog√≠a 4)** | Se utilizaron scripts para generar respaldos sin depender de una persona, lo que hace que el sistema trabaje solo. |

> En conjunto, estas tecnolog√≠as permiten crear un entorno con respaldos autom√°ticos y monitoreo constante, ideal para una universidad donde la informaci√≥n no puede perderse.

---

### 3.2 Temas de la Asignatura Aplicados (T1 - T6)

| Tema | Aplicaci√≥n en el sistema de copias de seguridad para la universidad |
|---|---|
| üü¢ **Continuidad Operacional (T1)** | Los respaldos autom√°ticos permiten recuperar informaci√≥n en caso de fallas, evitando detener tr√°mites, clases o informaci√≥n institucional. |
| üü¢ **Alta Disponibilidad y Tolerancia a Fallos (T2)** | Con la r√©plica Maestro‚ÄìEsclavo en PostgreSQL, los datos existen en dos servidores, reduciendo el riesgo de perderlos completamente. |
| üü¢ **Balanceo y Proxy (T3)** | Nginx ayuda a gestionar solicitudes, y el sistema puede escalar a m√°s instancias si en un futuro la universidad lo necesita. |
| üü¢ **Optimizaci√≥n y Servicios Web (T3/T4)** | Los servicios fueron organizados para que respondan de manera m√°s fluida, permitiendo que el sistema crezca con el tiempo. |
| üü¢ **Seguridad y Hardening (T5)** | Se configuraron reglas de firewall (UFW) para limitar el acceso solo a los puertos necesarios y proteger la informaci√≥n sensible. |
| üü¢ **Automatizaci√≥n (T6)** | Los respaldos se ejecutan solos con scripts Bash, reduciendo trabajo manual y evitando errores humanos. |

---

## III. IV. Dise√±o de la Infraestructura y Topolog√≠a

### 4.1 Dise√±o Esquem√°tico

| VM/Host | Rol | IP F√≠sica | IP Virtual | Red L√≥gica | SO |
|---|---|---|---|---|---|
| VM-WEB1 | Servidor Web 1 | 172.20.10.4 | N/A | Red 10 | Ubuntu 24.04 |
| VM-WEB2 | Servidor Web 2 | 172.20.10.10 | N/A | Red 10 | Ubuntu 24.04 |
| VM-BAL | Balanceador / Punto de Entrada | 172.20.10.5 | N/A | Red 15 | Ubuntu 24.04 |
| VM-DB-M | Base de Datos Maestro | 172.20.10.6 | N/A | Red 20 | Ubuntu 24.04 |
| VM-DB-S | Base de Datos Esclavo | 172.20.10.7 | N/A | Red 20 | Ubuntu 24.04 |
| VM-BACKUP | Servidor de Copias | 172.20.10.14 | N/A | Red 25 | Ubuntu 24.04 |

---

### 4.2 Estrategia Adoptada

**Estrategia de Replicaci√≥n:**  
Se opt√≥ por la replicaci√≥n as√≠ncrona de Postgres debido a la menor latencia, priorizando la separaci√≥n de lectura/escritura para mantener mayor estabilidad entre conexi√≥n y peticiones a la base de datos evitando que se sobrecargue.

**Estrategia de Hardening:**  
Se aplicaron los est√°ndares CIS de hardening para mantener una mayor seguridad en los sitios web evitando accesos no deseados mediante conexiones SSH, protegiendo de esa manera los servidores, tambi√©n aplicando protecci√≥n a los diferentes puertos mediante el firewall para una mayor seguridad.

---

## V. Gu√≠a de Implementaci√≥n y Puesta en Marcha

A continuaci√≥n se describen los pasos esenciales para replicar todo el sistema, incluyendo los requisitos previos, el proceso de despliegue y los archivos clave utilizados durante la configuraci√≥n.

---

### 5.1. Pre-requisitos

- 7 VMs con Ubuntu 24.04 y acceso root/sudo con el servicio de SSH activo.  
- Repositorio Git clonado en cada VM.  
- Conexi√≥n entre todas las m√°quinas virtuales dentro de la misma red o redes internas configuradas.

---

### 5.2. Despliegue (Ejecuci√≥n de la Automatizaci√≥n)

**Instalaci√≥n:**
- Instalar en 3 VMs la base de datos Postgres.
- Instalar en 2 VMs servidores con Node.js.
- Instalar en 1 VM el balanceador de cargas con Nginx.

**Configuraci√≥n:**  
- Entrar a cd /var/www/app/ 
- Ingresar al directorio de la aplicaci√≥n web.

**Ejecuci√≥n (opcional):**
```bash
sudo ./script.sh
```

---

### 5.3. Ficheros de Configuraci√≥n Clave

A continuaci√≥n se muestran los archivos y comandos utilizados durante la configuraci√≥n del sistema.  
Estos ficheros representan la estructura real utilizada dentro de las VMs:

Last login: Tue Dec 2 02:34:11 2025 from 172.20.10.3
admin1@admin1:~$ nano web1.sh
admin1@admin1:~$ cd /var/www/app/
admin1@admin1:/var/www/app$ ls
database.db
node_modules
package.json
package-lock.json
admin1@admin1:/var/www/app$ nano public/index/.html
admin1@admin1:/var/www/app$ sudo nano public/index.html
[sudo] password for admin1:

#### Configuraci√≥n del Balanceador (HAProxy)

log /dev/log local0
maxconn 4096
user haproxy
group haproxy

defaults
    log     global
    mode    http
    timeout connect 5000
    timeout client 50000
    timeout server 50000

frontend http_front
    bind *:80
    default_backend web_servers

backend web_servers
    balance roundrobin
    option httpchk GET /
    server vm1 172.20.10.4:80 check
    server vm2 172.20.10.10:80 check

Estos archivos forman parte esencial del sistema e incluyen:  
- Scripts de despliegue (web1.sh, web2.sh, balanceador.sh)  
- Archivos de la aplicaci√≥n alojados en /var/www/app/  
- Configuraci√≥n del balanceador HAProxy  
- Ediciones realizadas mediante nano en cada servidor

## VI. Pruebas y Validaci√≥n

A continuaci√≥n se presentan las pruebas realizadas al sistema, junto con lo que se esperaba obtener y el resultado obtenido durante las pruebas.

| Prueba Realizada | Resultado Esperado | Resultado Obtenido |
|------------------|--------------------|---------------------|
| **Test de Failover de la Base de Datos (Apagar Maestro)** | El esclavo debe tomar las escrituras o el servicio debe seguir activo. | OK |
| **Prueba de Carga/Estr√©s (Balanceo)** | El tr√°fico debe distribuirse entre los servidores web. | OK |
| **Test de Seguridad (SSL/Firewall)** | El acceso HTTP debe redirigir a HTTPS y el firewall debe bloquear todos los puertos excepto 443. | FALLIDO |
| **Ejecuci√≥n del Script del Servidor Web** | El script debe crear la aplicaci√≥n, instalar dependencias y generar los archivos necesarios. | OK |
| **Renderizado del Formulario HTML** | El formulario debe visualizarse correctamente desde el navegador. | OK |

## VII. Conclusiones y Lecciones Aprendidas

Este proyecto nos ayud√≥ a entender mejor c√≥mo funciona un sistema real donde varias m√°quinas trabajan juntas. Pudimos ver de primera mano lo importante que es tener respaldos autom√°ticos, ya que cualquier falla o ca√≠da puede dejar sin servicio a un mont√≥n de personas si no hay una copia funcionando.

Tambi√©n aprendimos que no basta con ‚Äúinstalar cosas‚Äù. Cada servicio necesita configuraciones espec√≠ficas, permisos, puertos abiertos, rutas correctas y bastante orden para que todo funcione como deber√≠a. Uno de los mayores retos fue darnos cuenta de que si una VM est√° mal configurada, todo lo dem√°s puede fallar, as√≠ que la coordinaci√≥n entre los servicios es clave.

En lo t√©cnico, muchas cosas salieron bien: los scripts del servidor web funcionaron, la aplicaci√≥n se levant√≥ sin problemas y varias configuraciones se aplicaron como se esperaba. Sin embargo, tambi√©n hubo cosas que no logramos al 100%, como el failover y el balanceo de carga, que requieren una configuraci√≥n m√°s avanzada y m√°s tiempo de pruebas.

Si tuvi√©ramos que repetir este proyecto, probablemente empezar√≠amos armando mejor las redes desde cero y organizando los roles de cada m√°quina con m√°s detalle desde el inicio. Tambi√©n planificar√≠amos mejor la parte de seguridad para no tener que hacer ajustes a √∫ltimo momento.

En general, el proyecto nos sirvi√≥ para aprender c√≥mo se arma un sistema que sea estable, seguro y f√°cil de mantener, y nos mostr√≥ la realidad de trabajar con infraestructura distribuida, donde cada cambio cuenta y todo debe estar bien conectado.





